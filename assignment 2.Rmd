---
title: "Assignment 2"
author: "Emily Leff"
date: "10/9/2019"
output: html_document
---

## Part 1

a) 
```{r pressure, echo=FALSE}
weather <- read.csv("weather.csv")

library(caret)
set.seed(1234)
index <- createDataPartition(weather$RainTomorrow, p = .8, list=FALSE)
training <- weather[index, ]
testing <- weather[-index, ]

fit.lr <- glm(RainTomorrow ~ ., data=training, family=binomial)
summary(fit.lr)
```

b) A 1 degree increase in maximum temperature multiplies the odds of rain tomorrow by 1.14 (e^0.13369). This is a 13% increase in odds.

c) Our model has an accuracy of 90%.
```{r}
testing$prob <- predict(fit.lr, testing, type="response")
testing$pred <- factor(testing$prob > .5, levels=c(FALSE, TRUE),
                        labels=c("No", "Yes"))

table(testing$RainTomorrow, testing$pred) 
confusionMatrix(testing$pred, testing$RainTomorrow, positive="Yes")
```
 d) There is a 13.5% probability that it will rain tomorrow given today's weather.
```{r}
newdata <- data.frame("MinTemp"=5.2, "MaxTemp" = 18.5, "Rainfall"=3.9,  "Evaporation"=2,
                      "Sunshine"= 5.1, "WindGustSpeed" = 31, "RainTomorrow"=NA)
newdata$prob <- predict(fit.lr, newdata, type="response")
newdata$pred <- factor(newdata$prob > .5, levels=c(FALSE, TRUE),
                        labels=c("No", "Yes"))   
print(newdata$prob)
```

e) The sensitivity is 54%, meaning that my model can correctly identify 54% of cases where it does rain tomorrow (prediction of "yes"). The specificity is 98%, meaning my model can correctly identify 98% of cases where it does not rain tomorrow (prediction of "no").

f) If I say that it will rain, there is an 88% chance that I am right (the positive predictive value is .875). If I say that it will not rain, there is a 91% chance that I am right (the negative predictive value is .906)

## Part 2

a)
```{r}
library(readxl)
blog <- read_excel("blog_gender.xlsx", col_names = FALSE)
blog <- setNames(blog, c("text","gender"))
var.keep <- c("text","gender")
blog <- blog[ ,var.keep]
blog <- na.omit(blog)
table(blog$gender)
blog$gender[blog$gender == "f"] <-"F"
blog$gender[blog$gender == "m"] <-"M"
blog$gender <- factor(blog$gender)


library(tm)
blog_corpus <- Corpus(VectorSource(blog$text))


corpus_clean <- tm_map(blog_corpus, tolower)
corpus_clean <- tm_map(corpus_clean, removeNumbers)
corpus_clean <- tm_map(corpus_clean, removeWords, stopwords())
corpus_clean <- tm_map(corpus_clean, removePunctuation)
corpus_clean <- tm_map(corpus_clean, stripWhitespace)


library(SnowballC)
corpus_clean <- tm_map(corpus_clean, stemDocument)


blog_dtm <- DocumentTermMatrix(corpus_clean)


blog_dtm_train <- blog_dtm[1:2258, ]
blog_dtm_test <- blog_dtm[2259:3226, ]

blog_train_labels <- blog$gender[1:2258]
blog_test_labels <- blog$gender[2259:3226]


blog_freq_words <- findFreqTerms(blog_dtm_train, 5)

blog_dtm_freq_train <- blog_dtm_train[, blog_freq_words]
blog_dtm_freq_test  <- blog_dtm_test[, blog_freq_words]


convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0)
  x <- factor(x, levels = c(0, 1), labels = c("No", "Yes"))
}


blog_train <- apply(blog_dtm_freq_train, 2, convert_counts)
blog_test  <- apply(blog_dtm_freq_test,  2, convert_counts)
```

b) My model is 64% accurate. 
```{r}
library(caret)
library(e1071)

blog_classifier <- naiveBayes(blog_train, blog_train_labels,
                             laplace=1)
blog_test_pred <- predict(blog_classifier, blog_test)

confusionMatrix(blog_test_labels, blog_test_pred, positive="M")
```

c) From a practical point of view, predicting the gender of a blog writer could shed light on the ratio of male to female blog writers on specific online platforms. This could be useful for an online platform with a specific political agenda and seeing if any relationship gender and politics, or online platforms with a specific vocation/theme (ie. cooking, fashion, sports) to see any dominant presence of men or women in those fields (and how that may change over time). From this prediction, you could also look at the amount of likes/views/hits specific blog posts get and if that correlates with gender, which has implications on personal "voice" in writing and what voices people are more drawn to. A speculative (potentially farther-fetched) example of using this type of model would be to track the process of young men who are radicalized on far-right internet forums (ie. Incels, white supremacist Reddit threads, etc) -- by predicting the gender of the post writer, it could be easier to pinpoint where the starting points of this type of internet radicalization occurs (ie. finding male-dominated Internet corners vs. spaces with more balanced gender ratios).